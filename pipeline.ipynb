{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEMP 天文光谱数据处理流水线\n",
    "这个 Jupyter Notebook 是处理CEMP星光谱数据的主控流程。\n",
    "它将按顺序调用`preprocessing_scripts`中的各个模块来执行数据处理的每一步。\n",
    "\n",
    "**重要提示:** 在运行之前,请确保您已经安装了所有必需的依赖库。您可以运行以下命令来安装它们:\n",
    "```bash\n",
    "pip install astropy pandas numpy matplotlib scipy torch pytorch-wavelets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 265 个FITS文件，开始提取...\n",
      "  正在处理文件 10/265...\n",
      "  正在处理文件 20/265...\n",
      "  正在处理文件 30/265...\n",
      "  正在处理文件 40/265...\n",
      "  正在处理文件 50/265...\n",
      "  正在处理文件 60/265...\n",
      "  正在处理光谱 70/265...\n",
      "  正在处理文件 80/265...\n",
      "  正在处理文件 90/265...\n",
      "  正在处理文件 100/265...\n",
      "  正在处理文件 110/265...\n",
      "  正在处理文件 120/265...\n",
      "  正在处理文件 130/265...\n",
      "  正在处理文件 140/265...\n",
      "  正在处理文件 150/265...\n",
      "  正在处理文件 160/265...\n",
      "  正在处理文件 170/265...\n",
      "  正在处理文件 180/265...\n",
      "  正在处理文件 190/265...\n",
      "  正在处理文件 200/265...\n",
      "  正在处理文件 210/265...\n",
      "  正在处理文件 220/265...\n",
      "  正在处理文件 230/265...\n",
      "  正在处理文件 240/265...\n",
      "  正在处理文件 250/265...\n",
      "  正在处理文件 260/265...\n",
      "提取完成！\n",
      "已随机选择以下 OBSIDs 用于可视化: [101515229, 220712171, 181808061, 335115245, 15416073]\n",
      "为步骤 'Step1_Extraction' 生成 5 个指定的可视化样本...\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_101515229.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_220712171.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_181808061.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_335115245.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_15416073.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from preprocessing_scripts import utils\n",
    "# --- 归一化配置 ---\n",
    "# 可选策略: 'spline_iterative', 'wavelet', 'quantile', 'conv_envelope', 'spline_binned_max', 'binned_percentile'\n",
    "\n",
    "NORM_METHOD = 'spline_iterative'\n",
    "# --- 去噪配置 ---\n",
    "# 可选策略: 'savgol', 'median', 'wavelet', 'polynomial', 'moving_average', 'weighted_moving_average', 'none'\n",
    "DENOISE_strategy = 'none'\n",
    "\n",
    "# 输入/输出目录\n",
    "FITS_DIR = 'unzipped_fits_100—/'\n",
    "REDSHIFT_FILE = 'removed_with_rv.csv' # 包含红移信息的文件\n",
    "OUTPUT_DIR = 'files/'\n",
    "FIGURE_DIR = f'figures_{NORM_METHOD}_{DENOISE_strategy}/'\n",
    "\n",
    "# 确保输出目录存在\n",
    "utils.ensure_dir(OUTPUT_DIR)\n",
    "utils.ensure_dir(FIGURE_DIR)\n",
    "\n",
    "# --- 读取标签数据 ---\n",
    "# 在流程开始时加载一次，以便在可视化时使用\n",
    "if os.path.exists(REDSHIFT_FILE):\n",
    "    labels_df = pd.read_csv(REDSHIFT_FILE)\n",
    "    # 使用 obsid 作为索引，方便快速查找\n",
    "    if 'obsid' in labels_df.columns:\n",
    "        labels_df.set_index('obsid', inplace=True)\n",
    "    else:\n",
    "        print(f\"警告: 标签文件 {REDSHIFT_FILE} 中缺少 'obsid' 列，无法在图表中显示标签。\")\n",
    "        labels_df = None\n",
    "else:\n",
    "    print(f\"警告: 标签文件 {REDSHIFT_FILE} 未找到，无法在图表中显示标签。\")\n",
    "    labels_df = None\n",
    "\n",
    "# 可视化参数\n",
    "NUM_VISUALIZATION_SAMPLES = 5 # 为每个步骤生成多少个可视化样本\n",
    "\n",
    "# 功能开关\n",
    "SAVE_INTERMEDIATE_FILES = False # 是否保存每个步骤的中间文件？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第1步: 从FITS文件提取光谱数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 265 个FITS文件，开始提取...\n",
      "  正在处理文件 10/265...\n",
      "  正在处理文件 20/265...\n",
      "  正在处理文件 30/265...\n",
      "  正在处理文件 40/265...\n",
      "  正在处理文件 50/265...\n",
      "  正在处理文件 60/265...\n",
      "  正在处理文件 70/265...\n",
      "  正在处理文件 80/265...\n",
      "  正在处理文件 90/265...\n",
      "  正在处理文件 100/265...\n",
      "  正在处理文件 110/265...\n",
      "  正在处理文件 120/265...\n",
      "  正在处理文件 130/265...\n",
      "  正在处理文件 140/265...\n",
      "  正在处理文件 150/265...\n",
      "  正在处理文件 160/265...\n",
      "  正在处理文件 170/265...\n",
      "  正在处理文件 180/265...\n",
      "  正在处理文件 190/265...\n",
      "  正在处理文件 200/265...\n",
      "  正在处理文件 210/265...\n",
      "  正在处理文件 220/265...\n",
      "  正在处理文件 230/265...\n",
      "  正在处理文件 240/265...\n",
      "  正在处理文件 250/265...\n",
      "  正在处理文件 260/265...\n",
      "提取完成！\n",
      "已随机选择以下 OBSIDs 用于可视化: [101515229, 220712171, 181808061, 335115245, 15416073]\n",
      "为步骤 'Step1_Extraction' 生成 5 个指定的可视化样本...\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_101515229.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_220712171.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_181808061.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_335115245.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step1_Extraction_obsid_15416073.pdf\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_scripts.step1_extraction import process_step as extract_step\n",
    "\n",
    "spectra_data_raw = extract_step(FITS_DIR)\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    utils.save_spectra_to_csv(spectra_data_raw, os.path.join(OUTPUT_DIR, 'step1_spectra_data.csv'), ['flux'])\n",
    "\n",
    "all_obsids = [spec['obsid'] for spec in spectra_data_raw]\n",
    "num_samples = min(NUM_VISUALIZATION_SAMPLES, len(all_obsids))\n",
    "obsids_for_visualization = random.sample(all_obsids, num_samples)\n",
    "print(f\"已随机选择以下 OBSIDs 用于可视化: {obsids_for_visualization}\")\n",
    "\n",
    "utils.visualize_single_step('Step1_Extraction', spectra_data_raw, obsids_for_visualization, FIGURE_DIR, labels_df=labels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2步: 红移校正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对 265 个光谱进行红移校正...\n",
      "  正在处理光谱 10/265...\n",
      "  正在处理光谱 20/265...\n",
      "  正在处理光谱 30/265...\n",
      "  正在处理光谱 40/265...\n",
      "  正在处理光谱 50/265...\n",
      "  正在处理光谱 60/265...\n",
      "  正在处理光谱 70/265...\n",
      "  正在处理光谱 80/265...\n",
      "  正在处理光谱 90/265...\n",
      "  正在处理光谱 100/265...\n",
      "  正在处理光谱 110/265...\n",
      "  正在处理光谱 120/265...\n",
      "  正在处理光谱 130/265...\n",
      "  正在处理光谱 140/265...\n",
      "  正在处理光谱 150/265...\n",
      "  正在处理光谱 160/265...\n",
      "  正在处理光谱 170/265...\n",
      "  正在处理光谱 180/265...\n",
      "  正在处理光谱 190/265...\n",
      "  正在处理光谱 200/265...\n",
      "  正在处理光谱 210/265...\n",
      "  正在处理光谱 220/265...\n",
      "  正在处理光谱 230/265...\n",
      "  正在处理光谱 240/265...\n",
      "  正在处理光谱 250/265...\n",
      "  正在处理光谱 260/265...\n",
      "红移校正完成！\n",
      "为步骤 'Step2_RedshiftCorrection' 生成 5 个指定的可视化样本...\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step2_RedshiftCorrection_obsid_101515229.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step2_RedshiftCorrection_obsid_220712171.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step2_RedshiftCorrection_obsid_181808061.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step2_RedshiftCorrection_obsid_335115245.pdf\n",
      "  -> 单步可视化图表已保存: figures_spline_iterative_none/Step2_RedshiftCorrection_obsid_15416073.pdf\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_scripts.step2_redshift_correction import process_step as redshift_step\n",
    "\n",
    "spectra_data_corrected = redshift_step(spectra_data_raw, REDSHIFT_FILE)\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    utils.save_spectra_to_csv(spectra_data_corrected, os.path.join(OUTPUT_DIR, 'step2_spectra_data_corrected.csv'), ['flux'])\n",
    "\n",
    "utils.visualize_single_step('Step2_RedshiftCorrection', spectra_data_corrected, obsids_for_visualization, FIGURE_DIR, labels_df=labels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3步: 光谱去噪\n",
    "在此单元格中配置去噪策略和相关参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 去噪配置 ---\n",
    "# 可选策略: 'savgol', 'median', 'wavelet', 'polynomial', 'moving_average', 'weighted_moving_average', 'none'\n",
    "DENOISE_strategy = DENOISE_strategy\n",
    "\n",
    "# 为每种策略定义参数\n",
    "DENOISE_PARAMS = {\n",
    "    'savgol': {'window_length': 11, 'polyorder': 3},\n",
    "    'median': {'kernel_size': 3},\n",
    "    'wavelet': {'wavelet': 'db4', 'level': 4, 'device': 'cpu'},\n",
    "    'polynomial': {'degree': 5, 'threshold': 3.0},\n",
    "    'moving_average': {'window_size': 5},\n",
    "    'weighted_moving_average': {'weights': (0.25, 0.5, 0.25)},\n",
    "    'none': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对 265 个光谱进行去噪 (策略: none)...
",
      "  -> 'none' 策略被选中，将跳过实际去噪操作。
",
      "  正在处理光谱 10/265...
",
      "  正在处理光谱 20/265...
",
      "  正在处理光谱 30/265...
",
      "  正在处理光谱 40/265...
",
      "  正在处理光谱 50/265...
",
      "  正在处理光谱 60/265...
",
      "  正在处理光谱 70/265...
",
      "  正在处理光谱 80/265...
",
      "  正在处理光谱 90/265...
",
      "  正在处理光谱 100/265...
",
      "  正在处理光谱 110/265...
",
      "  正在处理光谱 120/265...
",
      "  正在处理光谱 130/265...
",
      "  正在处理光谱 140/265...
",
      "  正在处理光谱 150/265...
",
      "  正在处理光谱 160/265...
",
      "  正在处理光谱 170/265...
",
      "  正在处理光谱 180/265...
",
      "  正在处理光谱 190/265...
",
      "  正在处理光谱 200/265...
",
      "  正在处理光谱 210/265...
",
      "  正在处理光谱 220/265...
",
      "  正在处理光谱 230/265...
",
      "  正在处理光谱 240/265...
",
      "  正在处理光谱 250/265...
",
      "  正在处理光谱 260/265...
",
      "光谱去噪完成！\n",
      "为步骤 'Step3_Denoising_none' 生成 5 个指定的对比可视化样本...\n",
      "  -> 重叠对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_101515229_overlap.pdf\n",
      "  -> 子图对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_101515229_subplots.pdf\n",
      "  -> 重叠对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_220712171_overlap.pdf\n",
      "  -> 子图对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_220712171_subplots.pdf\n",
      "  -> 重叠对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_181808061_overlap.pdf\n",
      "  -> 子图对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_181808061_subplots.pdf\n",
      "  -> 重叠对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_335115245_overlap.pdf\n",
      "  -> 子图对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_335115245_subplots.pdf\n",
      "  -> 重叠对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_15416073_overlap.pdf\n",
      "  -> 子图对比图已保存: figures_spline_iterative_none/Step3_Denoising_none_obsid_15416073_subplots.pdf\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_scripts.step3_noise_removal import process_step as denoise_step\n",
    "\n",
    "denoise_params_for_step = DENOISE_PARAMS.get(DENOISE_strategy, { })\n",
    "\n",
    "spectra_data_denoised = denoise_step(\n",
    "    spectra_data_corrected, \n",
    "    strategy=DENOISE_strategy, \n",
    "    **denoise_params_for_step\n",
    ")\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    utils.save_spectra_to_csv(spectra_data_denoised, os.path.join(OUTPUT_DIR, f'step3_spectra_data_denoised_{DENOISE_strategy}.csv'), ['flux', 'flux_denoised'])\n",
    "\n",
    "utils.visualize_comparison_step(\n",
    "    step_name=f'Step3_Denoising_{DENOISE_strategy}', \n",
    "    before_dataset=spectra_data_corrected, \n",
    "    after_dataset=spectra_data_denoised, \n",
    "    obsids_to_visualize=obsids_for_visualization, \n",
    "    before_key='flux', \n",
    "    after_key='flux_denoised', \n",
    "    before_label='Original Flux', \n",
    "    after_label=f'Denoised Flux ({DENOISE_strategy})', \n",
    "    figure_dir=FIGURE_DIR,\n",
    "    labels_df=labels_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第4步: 连续谱归一化\n",
    "在此单元格中配置归一化策略和相关参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 归一化配置 ---\n",
    "# 可选策略: 'spline_iterative', 'wavelet', 'quantile', 'conv_envelope', 'spline_binned_max', 'binned_percentile'\n",
    "NORM_METHOD = NORM_METHOD\n",
    "\n",
    "# 为每种归一化策略定义参数\n",
    "NORM_PARAMS = {\n",
    "    'spline_iterative': {\n",
    "        'lower_sigma': 1.5, \n",
    "        'upper_sigma': 3.0, \n",
    "        'max_iter': 10,     \n",
    "        'spline_k': 3,      \n",
    "        'spline_s_factor': 1.0 \n",
    "    },\n",
    "    'wavelet': {\n",
    "        'wavelet': 'db8',   \n",
    "        'level': 5,         \n",
    "        'device': 'cuda'   \n",
    "    },\n",
    "    'quantile': {\n",
    "        'window_size': 101, \n",
    "        'quantile': 0.95,   \n",
    "        'smooth_window': 0 \n",
    "    },\n",
    "    'conv_envelope': {\n",
    "        'median_window': 51,    \n",
    "        'max_window': 51,       \n",
    "        'smooth_window': 51     \n",
    "    },\n",
    "    'spline_binned_max': {\n",
    "        'num_bins': 50, \n",
    "        'spline_k': 3 \n",
    "    },\n",
    "    'binned_percentile': {\n",
    "        'bin_size': 150, \n",
    "        'percentile': 85, \n",
    "        'spline_k': 3 \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对 265 个光谱进行归一化 (策略: spline_iterative)...
"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irving/workspace/CEMP_data/preprocessing_scripts/step4_normalization.py:41: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  spline = UnivariateSpline(wavelength[mask], flux[mask], k=spline_k, s=len(wavelength[mask]) * spline_s_factor)
"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  正在处理光谱 10/265...
",
      "  正在处理光谱 20/265...
"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irving/workspace/CEMP_data/preprocessing_scripts/step4_normalization.py:41: UserWarning: \n",
      "A theoretically impossible result was found during the iteration\n",
      "process for finding a smoothing spline with fp = s: s too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  spline = UnivariateSpline(wavelength[mask], flux[mask], k=spline_k, s=len(wavelength[mask]) * spline_s_factor)
"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  正在处理光谱 30/265...
",
      "  正在处理光谱 40/265...
",
      "  正在处理光谱 50/265...
"
     ]
    }
   ],
   "source": [
    "from preprocessing_scripts.step4_normalization import process_step as normalize_step\n",
    "\n",
    "norm_params_for_step = NORM_PARAMS.get(NORM_METHOD, { })\n",
    "\n",
    "spectra_data_normalized = normalize_step(\n",
    "    spectra_data_denoised, \n",
    "    method=NORM_METHOD, \n",
    "    **norm_params_for_step\n",
    ")\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    utils.save_spectra_to_csv(spectra_data_normalized, os.path.join(OUTPUT_DIR, f'step4_spectra_data_normalized_{NORM_METHOD}.csv'), ['flux_normalized'])\n",
    "    utils.save_spectra_to_csv(spectra_data_normalized, os.path.join(OUTPUT_DIR, f'step4_spectra_data_continuum_{NORM_METHOD}.csv'), ['continuum'])\n",
    "\n",
    "utils.visualize_continuum_fit(spectra_data_normalized, obsids_for_visualization, FIGURE_DIR, labels_df=labels_df)\n",
    "utils.visualize_normalization_step(f'Step4_Normalization_{NORM_METHOD}', spectra_data_normalized, obsids_for_visualization, FIGURE_DIR, labels_df=labels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第5步: 光谱重采样与格式化\n",
    "在此单元格中配置最终的波长网格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 重采样配置 ---\n",
    "# 定义一个或多个波长区间及其采样步长 (Å)。格式: (起始波长, 结束波长, 步长)\n",
    "WAVELENGTH_CONFIG = [\n",
    "    (3800, 5700, 1), # 蓝端，较高分辨率采样\n",
    "    (5900, 8800, 1)  # 红端，较低分辨率采样\n",
    "]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对 265 个光谱进行重采样和格式化...\n",
      "  -> 已生成包含 4800 个数据点的最终波长网格。\n",
      "  正在处理光谱 10/265...\n",
      "  正在处理光谱 20/265...\n",
      "  正在处理光谱 30/265...\n",
      "  正在处理光谱 40/265...\n",
      "  正在处理光谱 50/265...\n",
      "  正在处理光谱 60/265...\n",
      "  正在处理光谱 70/265...\n",
      "  正在处理光谱 80/265...\n",
      "  正在处理光谱 90/265...\n",
      "  正在处理光谱 100/265...\n",
      "  正在处理光谱 110/265...\n",
      "  正在处理光谱 120/265...\n",
      "  正在处理光谱 130/265...\n",
      "  正在处理光谱 140/265...\n",
      "  正在处理光谱 150/265...\n",
      "  正在处理光谱 160/265...\n",
      "  正在处理光谱 170/265...\n",
      "  正在处理光谱 180/265...\n",
      "  正在处理光谱 190/265...\n",
      "  正在处理光谱 200/265...\n",
      "  正在处理光谱 210/265...\n",
      "  正在处理光谱 220/265...\n",
      "  正在处理光谱 230/265...\n",
      "  正在处理光谱 240/265...\n",
      "  正在处理光谱 250/265...\n",
      "  正在处理光谱 260/265...\n",
      "光谱重采样和格式化完成！\n",
      "  -> 最终数据产品已保存到: files/final_spectra_normalized_none_quantile_4800.csv\n",
      "  -> 最终数据产品已保存到: files/final_spectra_continuum_none_quantile_4800.csv\n",
      "对最终生成的数据产品进行可视化抽查：\n",
      "为最终数据产品 'Normalized Spectra' 生成 5 个可视化样本...\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Normalized Spectra_obsid_152716250.pdf\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Normalized Spectra_obsid_339611163.pdf\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Normalized Spectra_obsid_203108017.pdf\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Normalized Spectra_obsid_220305224.pdf\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Normalized Spectra_obsid_159704174.pdf\n",
      "为最终数据产品 'Continuum Spectra' 生成 5 个可视化样本...\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Continuum Spectra_obsid_152716250.pdf\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Continuum Spectra_obsid_339611163.pdf\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Continuum Spectra_obsid_203108017.pdf\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Continuum Spectra_obsid_220305224.pdf\n",
      "  -> 最终产品可视化图表已保存: figures_quantile_none/Final_Spectrum_Continuum Spectra_obsid_159704174.pdf\n",
      "开始计算连续谱的全面统计数据...\n",
      "连续谱的统计数据已保存到: files/continuum_stats_none_quantile_4800.yaml\n",
      "流水线所有步骤执行完毕！\n"
     ]
    }
   ],
   "source": [
    "from preprocessing_scripts.step5_resample import process_step as resample_step\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "# --- 执行重采样与格式化 ---\n",
    "df_normalized_final, df_continuum_final = resample_step(spectra_data_normalized, WAVELENGTH_CONFIG)\n",
    "\n",
    "# --- 保存最终的数据产品 ---\n",
    "utils.save_dataframe_to_csv(df_normalized_final, os.path.join(OUTPUT_DIR, f'final_spectra_normalized_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.csv'))\n",
    "utils.save_dataframe_to_csv(df_continuum_final, os.path.join(OUTPUT_DIR, f'final_spectra_continuum_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.csv'))\n",
    "\n",
    "# --- 可视化最终产品 ---\n",
    "print('对最终生成的数据产品进行可视化抽查：')\n",
    "utils.visualize_final_spectra(df_normalized_final, obsids_for_visualization, FIGURE_DIR, 'Normalized Spectra', labels_df=labels_df)\n",
    "utils.visualize_final_spectra(df_continuum_final, obsids_for_visualization, FIGURE_DIR, 'Continuum Spectra', labels_df=labels_df)\n",
    "\n",
    "# --- 计算全面的统计数据 ---\n",
    "print('开始计算连续谱的全面统计数据...')\n",
    "numeric_df = df_continuum_final.select_dtypes(include=np.number)\n",
    "all_values = numeric_df.values.flatten()\n",
    "\n",
    "# 计算统计量\n",
    "stats = {\n",
    "    'mean': float(np.mean(all_values)),\n",
    "    'variance': float(np.var(all_values)),\n",
    "    'std_dev': float(np.std(all_values)),\n",
    "    'min': float(np.min(all_values)),\n",
    "    'max': float(np.max(all_values)),\n",
    "    '25th_percentile': float(np.percentile(all_values, 25)),\n",
    "    'median_50th_percentile': float(np.median(all_values)),\n",
    "    '75th_percentile': float(np.percentile(all_values, 75))\n",
    "}\n",
    "\n",
    "# 保存到YAML文件\n",
    "stats_file_path = os.path.join(OUTPUT_DIR, f'continuum_stats_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.yaml')\n",
    "with open(stats_file_path, 'w') as f:\n",
    "    yaml.dump(stats, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"连续谱的统计数据已保存到: {stats_file_path}\")\n",
    "\n",
    "print('流水线所有步骤执行完毕！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算吸收线谱的全面统计数据...\n",
      "连续谱的统计数据已保存到: files/normalized_stats_none_quantile_4800.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 计算全面的统计数据 ---\n",
    "print('开始计算吸收线谱的全面统计数据...')\n",
    "numeric_df = df_normalized_final.select_dtypes(include=np.number)\n",
    "all_values = numeric_df.values.flatten()\n",
    "\n",
    "# 计算统计量\n",
    "stats = {\n",
    "    'mean': float(np.mean(all_values)),\n",
    "    'variance': float(np.var(all_values)),\n",
    "    'std_dev': float(np.std(all_values)),\n",
    "    'min': float(np.min(all_values)),\n",
    "    'max': float(np.max(all_values)),\n",
    "    '25th_percentile': float(np.percentile(all_values, 25)),\n",
    "    'median_50th_percentile': float(np.median(all_values)),\n",
    "    '75th_percentile': float(np.percentile(all_values, 75))\n",
    "}\n",
    "\n",
    "# 保存到YAML文件\n",
    "stats_file_path = os.path.join(OUTPUT_DIR, f'normalized_stats_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.yaml')\n",
    "with open(stats_file_path, 'w') as f:\n",
    "    yaml.dump(stats, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"连续谱的统计数据已保存到: {stats_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}