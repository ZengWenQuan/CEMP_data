{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEMP 天文光谱数据处理流水线\n",
    "这个 Jupyter Notebook 是处理CEMP星光谱数据的主控流程。\n",
    "它将按顺序调用`preprocessing_scripts`中的各个模块来执行数据处理的每一步。\n",
    "\n",
    "**重要提示:** 在运行之前,请确保您已经安装了所有必需的依赖库。您可以运行以下命令来安装它们:\n",
    "```bash\n",
    "pip install astropy pandas numpy matplotlib scipy torch pytorch-wavelets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from preprocessing_scripts import utils\n",
    "# --- 归一化配置 ---\n",
    "# 可选策略: 'spline_iterative', 'wavelet', 'moving_percentile', 'conv_envelope'\n",
    "\n",
    "NORM_METHOD = 'conv_envelope'\n",
    "# --- 去噪配置 ---\n",
    "# 可选策略: 'savgol', 'median', 'wavelet', 'polynomial', 'moving_average', 'weighted_moving_average', 'none'\n",
    "DENOISE_strategy = 'none'\n",
    "\n",
    "# 输入/输出目录\n",
    "FITS_DIR = 'unzipped_fits_100—/'\n",
    "REDSHIFT_FILE = 'removed_with_rv.csv' # 包含红移信息的文件\n",
    "OUTPUT_DIR = 'files/'\n",
    "FIGURE_DIR = f'figures/{NORM_METHOD}_{DENOISE_strategy}/'\n",
    "\n",
    "# 确保输出目录存在\n",
    "utils.ensure_dir(OUTPUT_DIR)\n",
    "utils.ensure_dir(FIGURE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 读取标签数据 ---\n",
    "# 在流程开始时加载一次，以便在可视化时使用\n",
    "if os.path.exists(REDSHIFT_FILE):\n",
    "    labels_df = pd.read_csv(REDSHIFT_FILE)\n",
    "    # 使用 obsid 作为索引，方便快速查找\n",
    "    if 'obsid' in labels_df.columns:\n",
    "        labels_df.set_index('obsid', inplace=True)\n",
    "    else:\n",
    "        print(f\"警告: 标签文件 {REDSHIFT_FILE} 中缺少 'obsid' 列，无法在图表中显示标签。\")\n",
    "        labels_df = None\n",
    "else:\n",
    "    print(f\"警告: 标签文件 {REDSHIFT_FILE} 未找到，无法在图表中显示标签。\")\n",
    "    labels_df = None\n",
    "\n",
    "# 可视化参数\n",
    "NUM_VISUALIZATION_SAMPLES = 5 # 为每个步骤生成多少个可视化样本\n",
    "\n",
    "# 功能开关\n",
    "SAVE_INTERMEDIATE_FILES = False # 是否保存每个步骤的中间文件？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 265 个FITS文件，开始提取...\n",
      "  正在处理文件 10/265...\n",
      "  正在处理文件 20/265...\n",
      "  正在处理文件 30/265...\n",
      "  正在处理文件 40/265...\n",
      "  正在处理文件 50/265...\n",
      "  正在处理文件 60/265...\n",
      "  正在处理文件 70/265...\n",
      "  正在处理文件 80/265...\n",
      "  正在处理文件 90/265...\n",
      "  正在处理文件 100/265...\n",
      "  正在处理文件 110/265...\n",
      "  正在处理文件 120/265...\n",
      "  正在处理文件 130/265...\n",
      "  正在处理文件 140/265...\n",
      "  正在处理文件 150/265...\n",
      "  正在处理文件 160/265...\n",
      "  正在处理文件 170/265...\n",
      "  正在处理文件 180/265...\n",
      "  正在处理文件 190/265...\n",
      "  正在处理文件 200/265...\n",
      "  正在处理文件 210/265...\n",
      "  正在处理文件 220/265...\n",
      "  正在处理文件 230/265...\n",
      "  正在处理文件 240/265...\n",
      "  正在处理文件 250/265...\n",
      "  正在处理文件 260/265...\n",
      "提取完成！\n",
      "已随机选择以下 OBSIDs 用于可视化: [161416083, 102711125, 334505164, 239104170, 139202101]\n",
      "为步骤 'Step1_Extraction' 生成 5 个指定的可视化样本...\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step1_Extraction_obsid_161416083.pdf\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step1_Extraction_obsid_102711125.pdf\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step1_Extraction_obsid_334505164.pdf\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step1_Extraction_obsid_239104170.pdf\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step1_Extraction_obsid_139202101.pdf\n",
      "开始对 265 个光谱进行红移校正...\n",
      "  正在处理光谱 10/265...\n",
      "  正在处理光谱 20/265...\n",
      "  正在处理光谱 30/265...\n",
      "  正在处理光谱 40/265...\n",
      "  正在处理光谱 50/265...\n",
      "  正在处理光谱 60/265...\n",
      "  正在处理光谱 70/265...\n",
      "  正在处理光谱 80/265...\n",
      "  正在处理光谱 90/265...\n",
      "  正在处理光谱 100/265...\n",
      "  正在处理光谱 110/265...\n",
      "  正在处理光谱 120/265...\n",
      "  正在处理光谱 130/265...\n",
      "  正在处理光谱 140/265...\n",
      "  正在处理光谱 150/265...\n",
      "  正在处理光谱 160/265...\n",
      "  正在处理光谱 170/265...\n",
      "  正在处理光谱 180/265...\n",
      "  正在处理光谱 190/265...\n",
      "  正在处理光谱 200/265...\n",
      "  正在处理光谱 210/265...\n",
      "  正在处理光谱 220/265...\n",
      "  正在处理光谱 230/265...\n",
      "  正在处理光谱 240/265...\n",
      "  正在处理光谱 250/265...\n",
      "  正在处理光谱 260/265...\n",
      "红移校正完成！\n",
      "为步骤 'Step2_RedshiftCorrection' 生成 5 个指定的可视化样本...\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step2_RedshiftCorrection_obsid_161416083.pdf\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step2_RedshiftCorrection_obsid_102711125.pdf\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step2_RedshiftCorrection_obsid_334505164.pdf\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step2_RedshiftCorrection_obsid_239104170.pdf\n",
      "  -> 单步可视化图表已保存: figures/conv_envelope_none/Step2_RedshiftCorrection_obsid_139202101.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 提取光谱数据 ---\n",
    "from preprocessing_scripts.step1_extraction import process_step as extract_step\n",
    "\n",
    "spectra_data_raw = extract_step(FITS_DIR)\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    utils.save_spectra_to_csv(spectra_data_raw, os.path.join(OUTPUT_DIR, 'step1_spectra_data.csv'), ['flux'])\n",
    "\n",
    "all_obsids = [spec['obsid'] for spec in spectra_data_raw]\n",
    "num_samples = min(NUM_VISUALIZATION_SAMPLES, len(all_obsids))\n",
    "obsids_for_visualization = random.sample(all_obsids, num_samples)\n",
    "print(f\"已随机选择以下 OBSIDs 用于可视化: {obsids_for_visualization}\")\n",
    "\n",
    "utils.visualize_single_step('Step1_Extraction', spectra_data_raw, obsids_for_visualization, FIGURE_DIR, labels_df=labels_df)\n",
    "\n",
    "# --- 红移校正 ---\n",
    "from preprocessing_scripts.step2_redshift_correction import process_step as redshift_step\n",
    "\n",
    "spectra_data_corrected = redshift_step(spectra_data_raw, REDSHIFT_FILE)\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    utils.save_spectra_to_csv(spectra_data_corrected, os.path.join(OUTPUT_DIR, 'step2_spectra_data_corrected.csv'), ['flux'])\n",
    "\n",
    "utils.visualize_single_step('Step2_RedshiftCorrection', spectra_data_corrected, obsids_for_visualization, FIGURE_DIR, labels_df=labels_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对 265 个光谱进行去噪 (策略: none)...\n",
      "  -> 'none' 策略被选中，将跳过实际去噪操作。\n",
      "  正在处理光谱 10/265...\n",
      "  正在处理光谱 20/265...\n",
      "  正在处理光谱 30/265...\n",
      "  正在处理光谱 40/265...\n",
      "  正在处理光谱 50/265...\n",
      "  正在处理光谱 60/265...\n",
      "  正在处理光谱 70/265...\n",
      "  正在处理光谱 80/265...\n",
      "  正在处理光谱 90/265...\n",
      "  正在处理光谱 100/265...\n",
      "  正在处理光谱 110/265...\n",
      "  正在处理光谱 120/265...\n",
      "  正在处理光谱 130/265...\n",
      "  正在处理光谱 140/265...\n",
      "  正在处理光谱 150/265...\n",
      "  正在处理光谱 160/265...\n",
      "  正在处理光谱 170/265...\n",
      "  正在处理光谱 180/265...\n",
      "  正在处理光谱 190/265...\n",
      "  正在处理光谱 200/265...\n",
      "  正在处理光谱 210/265...\n",
      "  正在处理光谱 220/265...\n",
      "  正在处理光谱 230/265...\n",
      "  正在处理光谱 240/265...\n",
      "  正在处理光谱 250/265...\n",
      "  正在处理光谱 260/265...\n",
      "光谱去噪完成！\n",
      "为步骤 'Step3_Denoising_none' 生成 5 个指定的对比可视化样本...\n",
      "  -> 重叠对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_161416083_overlap.pdf\n",
      "  -> 子图对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_161416083_subplots.pdf\n",
      "  -> 重叠对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_102711125_overlap.pdf\n",
      "  -> 子图对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_102711125_subplots.pdf\n",
      "  -> 重叠对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_334505164_overlap.pdf\n",
      "  -> 子图对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_334505164_subplots.pdf\n",
      "  -> 重叠对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_239104170_overlap.pdf\n",
      "  -> 子图对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_239104170_subplots.pdf\n",
      "  -> 重叠对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_139202101_overlap.pdf\n",
      "  -> 子图对比图已保存: figures/conv_envelope_none/Step3_Denoising_none_obsid_139202101_subplots.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 光谱去噪\n",
    "# 在此单元格中配置去噪策略和相关参数。\n",
    "# --- 去噪配置 ---\n",
    "# 可选策略: 'savgol', 'median', 'wavelet', 'polynomial', 'moving_average', 'weighted_moving_average', 'none'\n",
    "DENOISE_strategy = DENOISE_strategy\n",
    "\n",
    "# 为每种策略定义参数\n",
    "DENOISE_PARAMS = {\n",
    "    'savgol': {'window_length': 11, 'polyorder': 3},\n",
    "    'median': {'kernel_size': 3},\n",
    "    'wavelet': {'wavelet': 'db4', 'level': 4, 'device': 'cpu'},\n",
    "    'polynomial': {'degree': 5, 'threshold': 3.0},\n",
    "    'moving_average': {'window_size': 5},\n",
    "    'weighted_moving_average': {'weights': (0.25, 0.5, 0.25)},\n",
    "    'none': {}\n",
    "}\n",
    "from preprocessing_scripts.step3_noise_removal import process_step as denoise_step\n",
    "\n",
    "denoise_params_for_step = DENOISE_PARAMS.get(DENOISE_strategy, { })\n",
    "\n",
    "spectra_data_denoised = denoise_step(\n",
    "    spectra_data_corrected, \n",
    "    strategy=DENOISE_strategy, \n",
    "    **denoise_params_for_step\n",
    "    )\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    utils.save_spectra_to_csv(spectra_data_denoised, os.path.join(OUTPUT_DIR, f'step3_spectra_data_denoised_{DENOISE_strategy}.csv'), ['flux', 'flux_denoised'])\n",
    "\n",
    "utils.visualize_comparison_step(\n",
    "    step_name=f'Step3_Denoising_{DENOISE_strategy}', \n",
    "    before_dataset=spectra_data_corrected, \n",
    "    after_dataset=spectra_data_denoised, \n",
    "    obsids_to_visualize=obsids_for_visualization, \n",
    "    before_key='flux', \n",
    "    after_key='flux_denoised', \n",
    "    before_label='Original Flux', \n",
    "    after_label=f'Denoised Flux ({DENOISE_strategy})', \n",
    "    figure_dir=FIGURE_DIR,\n",
    "    labels_df=labels_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对 265 个光谱进行归一化 (策略: conv_envelope)...\n",
      "  -> 将对所有连续谱应用平滑处理 (方法: savgol, 窗口: 75)\n",
      "  正在处理光谱 10/265...\n",
      "  正在处理光谱 20/265...\n",
      "  正在处理光谱 30/265...\n",
      "  正在处理光谱 40/265...\n",
      "  正在处理光谱 50/265...\n",
      "  正在处理光谱 60/265...\n",
      "  正在处理光谱 70/265...\n",
      "  正在处理光谱 80/265...\n",
      "  正在处理光谱 90/265...\n",
      "  正在处理光谱 100/265...\n",
      "  正在处理光谱 110/265...\n",
      "  正在处理光谱 120/265...\n",
      "  正在处理光谱 130/265...\n",
      "  正在处理光谱 140/265...\n",
      "  正在处理光谱 150/265...\n",
      "  正在处理光谱 160/265...\n",
      "  正在处理光谱 170/265...\n",
      "  正在处理光谱 180/265...\n",
      "  正在处理光谱 190/265...\n",
      "  正在处理光谱 200/265...\n",
      "  正在处理光谱 210/265...\n",
      "  正在处理光谱 220/265...\n",
      "  正在处理光谱 230/265...\n",
      "  正在处理光谱 240/265...\n",
      "  正在处理光谱 250/265...\n",
      "  正在处理光谱 260/265...\n",
      "归一化完成！\n",
      "为连续谱拟合生成 5 个指定的可视化样本...\n",
      "  -> 连续谱拟合图已保存: figures/conv_envelope_none/Step4_ContinuumFit_obsid_161416083.pdf\n",
      "  -> 连续谱拟合图已保存: figures/conv_envelope_none/Step4_ContinuumFit_obsid_102711125.pdf\n",
      "  -> 连续谱拟合图已保存: figures/conv_envelope_none/Step4_ContinuumFit_obsid_334505164.pdf\n",
      "  -> 连续谱拟合图已保存: figures/conv_envelope_none/Step4_ContinuumFit_obsid_239104170.pdf\n",
      "  -> 连续谱拟合图已保存: figures/conv_envelope_none/Step4_ContinuumFit_obsid_139202101.pdf\n",
      "为步骤 'Step4_Normalization_conv_envelope' 生成 5 个指定的归一化可视化样本...\n",
      "  -> 归一化全流程对比图已保存: figures/conv_envelope_none/Step4_Normalization_conv_envelope_obsid_161416083_full_comparison.pdf\n",
      "  -> 归一化全流程对比图已保存: figures/conv_envelope_none/Step4_Normalization_conv_envelope_obsid_102711125_full_comparison.pdf\n",
      "  -> 归一化全流程对比图已保存: figures/conv_envelope_none/Step4_Normalization_conv_envelope_obsid_334505164_full_comparison.pdf\n",
      "  -> 归一化全流程对比图已保存: figures/conv_envelope_none/Step4_Normalization_conv_envelope_obsid_239104170_full_comparison.pdf\n",
      "  -> 归一化全流程对比图已保存: figures/conv_envelope_none/Step4_Normalization_conv_envelope_obsid_139202101_full_comparison.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 连续谱归一化\n",
    "# 在此单元格中配置归一化策略和相关参数。\n",
    "# --- 归一化配置 ---\n",
    "# 可选策略: 'spline_iterative', 'wavelet', 'moving_percentile', 'conv_envelope'\n",
    "NORM_METHOD = NORM_METHOD\n",
    "\n",
    "# 为每种归一化策略定义参数\n",
    "NORM_PARAMS = {\n",
    "    'spline_iterative': {\n",
    "        'lower_sigma': 1.5, \n",
    "        'upper_sigma': 3.0, \n",
    "        'max_iter': 10,     \n",
    "        'spline_k': 3,      \n",
    "        'spline_s_factor': 1.0 \n",
    "    },\n",
    "    'wavelet': {\n",
    "        'wavelet': 'db8',   \n",
    "        'level': 5,         \n",
    "        'device': 'cuda'   \n",
    "    },\n",
    "    'moving_percentile': {\n",
    "        'window_size': 101, \n",
    "        'percentile': 90  ,    \n",
    "    },\n",
    "    'conv_envelope': {\n",
    "        'median_window': 51,    \n",
    "        'max_window': 51,       \n",
    "        'smooth_window': 51     \n",
    "    }\n",
    "}\n",
    "from preprocessing_scripts.step4_normalization import process_step as normalize_step\n",
    "\n",
    "norm_params_for_step = NORM_PARAMS.get(NORM_METHOD, { })\n",
    "\n",
    "spectra_data_normalized = normalize_step(\n",
    "    spectra_data_denoised, \n",
    "    method=NORM_METHOD, \n",
    "    **norm_params_for_step\n",
    "    )\n",
    "\n",
    "if SAVE_INTERMEDIATE_FILES:\n",
    "    utils.save_spectra_to_csv(spectra_data_normalized, os.path.join(OUTPUT_DIR, f'step4_spectra_data_normalized_{NORM_METHOD}.csv'), ['flux_normalized'])\n",
    "    utils.save_spectra_to_csv(spectra_data_normalized, os.path.join(OUTPUT_DIR, f'step4_spectra_data_continuum_{NORM_METHOD}.csv'), ['continuum'])\n",
    "\n",
    "utils.visualize_continuum_fit(spectra_data_normalized, obsids_for_visualization, FIGURE_DIR, labels_df=labels_df)\n",
    "utils.visualize_normalization_step(f'Step4_Normalization_{NORM_METHOD}', spectra_data_normalized, obsids_for_visualization, FIGURE_DIR, labels_df=labels_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对 265 个光谱进行重采样和格式化...\n",
      "  -> 已生成包含 4800 个数据点的最终波长网格。\n",
      "  正在处理光谱 10/265...\n",
      "  正在处理光谱 20/265...\n",
      "  正在处理光谱 30/265...\n",
      "  正在处理光谱 40/265...\n",
      "  正在处理光谱 50/265...\n",
      "  正在处理光谱 60/265...\n",
      "  正在处理光谱 70/265...\n",
      "  正在处理光谱 80/265...\n",
      "  正在处理光谱 90/265...\n",
      "  正在处理光谱 100/265...\n",
      "  正在处理光谱 110/265...\n",
      "  正在处理光谱 120/265...\n",
      "  正在处理光谱 130/265...\n",
      "  正在处理光谱 140/265...\n",
      "  正在处理光谱 150/265...\n",
      "  正在处理光谱 160/265...\n",
      "  正在处理光谱 170/265...\n",
      "  正在处理光谱 180/265...\n",
      "  正在处理光谱 190/265...\n",
      "  正在处理光谱 200/265...\n",
      "  正在处理光谱 210/265...\n",
      "  正在处理光谱 220/265...\n",
      "  正在处理光谱 230/265...\n",
      "  正在处理光谱 240/265...\n",
      "  正在处理光谱 250/265...\n",
      "  正在处理光谱 260/265...\n",
      "光谱重采样和格式化完成！\n",
      "  -> 最终数据产品已保存到: files/final_spectra_normalized_none_conv_envelope_4800.csv\n",
      "  -> 最终数据产品已保存到: files/final_spectra_continuum_none_conv_envelope_4800.csv\n",
      "对最终生成的数据产品进行可视化抽查：\n",
      "为最终数据产品 'Normalized Spectra' 生成 5 个可视化样本...\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Normalized Spectra_obsid_161416083.pdf\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Normalized Spectra_obsid_102711125.pdf\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Normalized Spectra_obsid_334505164.pdf\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Normalized Spectra_obsid_239104170.pdf\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Normalized Spectra_obsid_139202101.pdf\n",
      "为最终数据产品 'Continuum Spectra' 生成 5 个可视化样本...\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Continuum Spectra_obsid_161416083.pdf\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Continuum Spectra_obsid_102711125.pdf\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Continuum Spectra_obsid_334505164.pdf\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Continuum Spectra_obsid_239104170.pdf\n",
      "  -> 最终产品可视化图表已保存: figures/conv_envelope_none/Final_Spectrum_Continuum Spectra_obsid_139202101.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 光谱重采样与格式化\n",
    "# 在此单元格中配置最终的波长网格。\n",
    "# --- 重采样配置 ---\n",
    "# 定义一个或多个波长区间及其采样步长 (Å)。格式: (起始波长, 结束波长, 步长)\n",
    "WAVELENGTH_CONFIG = [\n",
    "    (3800, 5700, 1), # 蓝端，较高分辨率采样\n",
    "    (5900, 8800, 1)  # 红端，较低分辨率采样\n",
    "]\n",
    "from preprocessing_scripts.step5_resample import process_step as resample_step\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "# --- 执行重采样与格式化 ---\n",
    "df_normalized_final, df_continuum_final = resample_step(spectra_data_normalized, WAVELENGTH_CONFIG)\n",
    "\n",
    "# --- 保存最终的数据产品 ---\n",
    "utils.save_dataframe_to_csv(df_normalized_final, os.path.join(OUTPUT_DIR, f'final_spectra_normalized_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.csv'))\n",
    "utils.save_dataframe_to_csv(df_continuum_final, os.path.join(OUTPUT_DIR, f'final_spectra_continuum_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.csv'))\n",
    "\n",
    "# --- 可视化最终产品 ---\n",
    "print('对最终生成的数据产品进行可视化抽查：')\n",
    "utils.visualize_final_spectra(df_normalized_final, obsids_for_visualization, FIGURE_DIR, 'Normalized Spectra', labels_df=labels_df)\n",
    "utils.visualize_final_spectra(df_continuum_final, obsids_for_visualization, FIGURE_DIR, 'Continuum Spectra', labels_df=labels_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算连续谱的全面统计数据...\n",
      "连续谱的统计数据已保存到: files/continuum_stats_none_conv_envelope_4800.yaml\n",
      "流水线所有步骤执行完毕！\n",
      "开始计算吸收线谱的全面统计数据...\n",
      "连续谱的统计数据已保存到: files/normalized_stats_none_conv_envelope_4800.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 计算全面的统计数据 ---\n",
    "print('开始计算连续谱的全面统计数据...')\n",
    "numeric_df = df_continuum_final.select_dtypes(include=np.number)\n",
    "all_values = numeric_df.values.flatten()\n",
    "\n",
    "# 计算统计量\n",
    "stats = {\n",
    "    'mean': float(np.mean(all_values)),\n",
    "    'variance': float(np.var(all_values)),\n",
    "    'std_dev': float(np.std(all_values)),\n",
    "    'min': float(np.min(all_values)),\n",
    "    'max': float(np.max(all_values)),\n",
    "    '25th_percentile': float(np.percentile(all_values, 25)),\n",
    "    'median_50th_percentile': float(np.median(all_values)),\n",
    "    '75th_percentile': float(np.percentile(all_values, 75))\n",
    "}\n",
    "\n",
    "# 保存到YAML文件\n",
    "stats_file_path = os.path.join(OUTPUT_DIR, f'continuum_stats_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.yaml')\n",
    "with open(stats_file_path, 'w') as f:\n",
    "    yaml.dump(stats, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"连续谱的统计数据已保存到: {stats_file_path}\")\n",
    "\n",
    "print('流水线所有步骤执行完毕！')\n",
    "\n",
    "# --- 计算全面的统计数据 ---\n",
    "print('开始计算吸收线谱的全面统计数据...')\n",
    "numeric_df = df_normalized_final.select_dtypes(include=np.number)\n",
    "all_values = numeric_df.values.flatten()\n",
    "\n",
    "# 计算统计量\n",
    "stats = {\n",
    "    'mean': float(np.mean(all_values)),\n",
    "    'variance': float(np.var(all_values)),\n",
    "    'std_dev': float(np.std(all_values)),\n",
    "    'min': float(np.min(all_values)),\n",
    "    'max': float(np.max(all_values)),\n",
    "    '25th_percentile': float(np.percentile(all_values, 25)),\n",
    "    'median_50th_percentile': float(np.median(all_values)),\n",
    "    '75th_percentile': float(np.percentile(all_values, 75))\n",
    "}\n",
    "\n",
    "# 保存到YAML文件\n",
    "stats_file_path = os.path.join(OUTPUT_DIR, f'normalized_stats_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.yaml')\n",
    "with open(stats_file_path, 'w') as f:\n",
    "    yaml.dump(stats, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"连续谱的统计数据已保存到: {stats_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始进行数据集划分...\n",
      "Loading datasets...\n",
      "Datasets loaded successfully.\n",
      "Train set size: 200\n",
      "Validation set size: 25\n",
      "Test set size: 26\n",
      "Ensuring consistent order of samples across all files...\n",
      "Plotting distributions for train set...\n",
      "  -> Saved Teff distribution plot to split_data/train/Teff_distribution.png\n",
      "  -> Saved logg distribution plot to split_data/train/logg_distribution.png\n",
      "  -> Saved CFe distribution plot to split_data/train/CFe_distribution.png\n",
      "  -> Saved FeH distribution plot to split_data/train/FeH_distribution.png\n",
      "Plotting distributions for val set...\n",
      "  -> Saved Teff distribution plot to split_data/val/Teff_distribution.png\n",
      "  -> Saved logg distribution plot to split_data/val/logg_distribution.png\n",
      "  -> Saved CFe distribution plot to split_data/val/CFe_distribution.png\n",
      "  -> Saved FeH distribution plot to split_data/val/FeH_distribution.png\n",
      "Plotting distributions for test set...\n",
      "  -> Saved Teff distribution plot to split_data/test/Teff_distribution.png\n",
      "  -> Saved logg distribution plot to split_data/test/logg_distribution.png\n",
      "  -> Saved CFe distribution plot to split_data/test/CFe_distribution.png\n",
      "  -> Saved FeH distribution plot to split_data/test/FeH_distribution.png\n",
      "Saving split CSV files...\n",
      "All files saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 数据集划分 ---\n",
    "# 调用脚本，根据FeH值对数据集进行分层抽样划分\n",
    "print('开始进行数据集划分...')\n",
    "# 首先，获取上一步生成的文件名\n",
    "continuum_filename = os.path.join(OUTPUT_DIR, f'final_spectra_continuum_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.csv')\n",
    "normalized_filename = os.path.join(OUTPUT_DIR, f'final_spectra_normalized_{DENOISE_strategy}_{NORM_METHOD}_{df_normalized_final.shape[1]}.csv')\n",
    "labels_filename = 'removed_with_rv_filtered.csv'\n",
    "output_split_dir = 'split_data'\n",
    "\n",
    "from preprocessing_scripts.split_dataset_by_feh import split_dataset_by_feh\n",
    "split_dataset_by_feh(continuum_filename,normalized_path=normalized_filename,labels_path=labels_filename,output_dir=output_split_dir)\n",
    "# 构建并执行命令\n",
    "# command = f'python3 preprocessing_scripts/split_dataset_by_feh.py --continuum_path {continuum_filename} --normalized_path {normalized_filename} --labels_path {labels_filename} --output_dir {output_split_dir}'\n",
    "# print(f'Executing: {command}')\n",
    "# !{command}\n",
    "# print('数据集划分完成！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
