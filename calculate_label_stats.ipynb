{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading label data from: removed_with_rv.csv\n",
      "Label data loaded successfully.\n",
      "Calculating statistics for labels: Teff, logg, FeH, CFe, CH\n",
      "  - Calculated stats for 'Teff'\n",
      "  - Calculated stats for 'logg'\n",
      "  - Calculated stats for 'FeH'\n",
      "  - Calculated stats for 'CFe'\n",
      "  - Calculated stats for 'CH'\n",
      "Saving statistics to: files/label_stats.yaml\n",
      "Script finished successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "REDSHIFT_FILE = 'removed_with_rv.csv'\n",
    "OUTPUT_DIR = 'files'\n",
    "LABEL_COLUMNS = ['Teff', 'logg', 'FeH', 'CFe', 'CH']\n",
    "OUTPUT_FILENAME = 'label_stats.yaml'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Main Logic ---\n",
    "print(f\"Reading label data from: {REDSHIFT_FILE}\")\n",
    "labels_df = pd.read_csv(REDSHIFT_FILE)\n",
    "print(\"Label data loaded successfully.\")\n",
    "\n",
    "print(f\"Calculating statistics for labels: {', '.join(LABEL_COLUMNS)}\")\n",
    "\n",
    "label_stats = {}\n",
    "for col in LABEL_COLUMNS:\n",
    "    if col in labels_df.columns:\n",
    "        # Drop NaN values for accurate calculations\n",
    "        valid_data = labels_df[col].dropna()\n",
    "        \n",
    "        stats = {\n",
    "            'mean': float(valid_data.mean()),\n",
    "            'variance': float(valid_data.var()),\n",
    "            'std_dev': float(valid_data.std()),\n",
    "            'min': float(valid_data.min()),\n",
    "            'max': float(valid_data.max()),\n",
    "            '25th_percentile': float(valid_data.quantile(0.25)),\n",
    "            'median_50th_percentile': float(valid_data.median()),\n",
    "            '75th_percentile': float(valid_data.quantile(0.75))\n",
    "        }\n",
    "        label_stats[col] = stats\n",
    "        print(f\"  - Calculated stats for '{col}'\")\n",
    "    else:\n",
    "        print(f\"  - Warning: Column '{col}' not found in the file.\")\n",
    "\n",
    "# --- Save to YAML ---\n",
    "output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "print(f\"Saving statistics to: {output_path}\")\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    yaml.dump(label_stats, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"Script finished successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# --- 配置 ---\n",
    "# 在这里选择您在主流水线中使用的去噪和归一化策略\n",
    "# --- 去噪配置 ---\n",
    "# 可选策略: 'savgol', 'median', 'wavelet', 'polynomial', 'moving_average', 'weighted_moving_average', 'none'\n",
    "DENOISE_strategy = 'none'\n",
    "\n",
    "# --- 归一化配置 ---\n",
    "# 可选策略: 'spline_iterative', 'wavelet', 'quantile', 'conv_envelope', 'spline_binned_max', 'binned_percentile'\n",
    "NORM_METHOD = 'spline_iterative'\n",
    "\n",
    "# --- 其他配置 ---\n",
    "INPUT_DIR = 'files'\n",
    "# 注意：波长点数（如4800）需要与您流水线第五步生成的文件名匹配\n",
    "# 如果不确定，请查看 'files' 目录下的实际文件名\n",
    "WAVELENGTH_POINTS = 4800 \n",
    "\n",
    "\n",
    "def analyze_csv(file_path):\n",
    "    print(\"\\n" + "-"*50)"",
    "    print(f"开始分析文件: {file_path}")\n",
    "    print("-"*50)\n",
    "    \n",
    "    try:\n",
    "        # 读取CSV文件，第一行为列名\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # 检查数据是否为空\n",
    "        if df.empty:\n",
    "            print(\"CSV文件为空，无法进行统计分析。\")\n",
    "            return\n",
    "        \n",
    "        # 除了第一列的所有数据\n",
    "        data_to_analyze = df.iloc[:, 1:]\n",
    "        \n",
    "        # 检查是否有数据需要分析\n",
    "        if data_to_analyze.empty:\n",
    "            print(\"除第一列外没有其他数据可分析。\")\n",
    "            return\n",
    "        \n",
    "        # 计算每列的统计数据\n",
    "        column_stats = {\n",
    "            '最大值': data_to_analyze.max(),\n",
    "            '最小值': data_to_analyze.min(),\n",
    "            '均值': data_to_analyze.mean(),\n",
    "            '中位数': data_to_analyze.median(),\n",
    "            '标准差': data_to_analyze.std(),\n",
    "            '总和': data_to_analyze.sum(),\n",
    "            '数据点数量': data_to_analyze.count()\n",
    "        }\n",
    "        \n",
    "        # 将列统计结果转换为DataFrame\n",
    "        column_stats_df = pd.DataFrame(column_stats)\n",
    "        \n",
    "        # 计算所有数据的总体统计信息\n",
    "        all_values = data_to_analyze.to_numpy().flatten() # 性能更优\n",
    "        \n",
    "        overall_stats = {\n",
    "            '总体最大值': np.nanmax(all_values),\n",
    "            '总体最小值': np.nanmin(all_values),\n",
    "            '总体均值': np.nanmean(all_values),\n",
    "            '总体中位数': np.nanmedian(all_values),\n",
    "            '总体标准差': np.nanstd(all_values),\n",
    "            '总体总和': np.nansum(all_values),\n",
    "            '总体数据点数量': len(all_values[~np.isnan(all_values)])\n",
    "        }\n",
    "        \n",
    "        # 打印结果\n",
    "        print(f"分析的列数: {len(data_to_analyze.columns)}")\n",
    "        \n",
    "        print("\\n各列统计结果 (展示前5条和后5条):")\n",
    "        print(column_stats_df.round(4).head())\n",
    "        print("...")\n",
    "        print(column_stats_df.round(4).tail())\n",
    "        \n",
    "        print("\\n总体统计结果:")\n",
    "        for key, value in overall_stats.items():\n",
    "            print(f"{key}: {round(value, 4)}")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f"错误: 找不到文件 '{file_path}'\\n请检查上面的配置是否与您运行的主流水线(pipeline.ipynb)完全一致。")\n",
    "    except Exception as e:\n",
    "        print(f"分析过程中发生错误: {str(e)}")\n",
    "\n",
    "if __name__ == "__main__":\n",
    "    # 根据配置动态构建文件名\n",
    "    base_filename_template = \"final_spectra_{product}_{denoise}_{norm}_{points}.csv\"\n",
    "    \n",
    "    # 1. 分析归一化光谱文件\n",
    "    normalized_filename = base_filename_template.format(\n",
    "        product='normalized',\n",
    "        denoise=DENOISE_strategy,\n",
    "        norm=NORM_METHOD,\n",
    "        points=WAVELENGTH_POINTS\n",
    "    )\n",
    "    normalized_file_path = os.path.join(INPUT_DIR, normalized_filename)\n",
    "    analyze_csv(normalized_file_path)\n",
    "    \n",
    "    # 2. 分析连续谱文件\n",
    "    continuum_filename = base_filename_template.format(\n",
    "        product='continuum',\n",
    "        denoise=DENOISE_strategy,\n",
    "        norm=NORM_METHOD,\n",
    "        points=WAVELENGTH_POINTS\n",
    "    )\n",
    "    continuum_file_path = os.path.join(INPUT_DIR, continuum_filename)\n",
    "    analyze_csv(continuum_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}